{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wNjDKdQy35h"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRm-USlsHgEV"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt3igws3eiVp"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pytorch-CycleGAN-and-pix2pix/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-450078e766ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch-CycleGAN-and-pix2pix/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pytorch-CycleGAN-and-pix2pix/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1EySlOXwwoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch>=1.4.0\n",
      "  Downloading torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 48 kB/s  eta 0:00:011     |████▍                           | 121.8 MB 25.4 MB/s eta 0:00:30     |█████▍                          | 148.0 MB 28.2 MB/s eta 0:00:27     |█████▍                          | 149.5 MB 28.2 MB/s eta 0:00:26     |██████████                      | 275.1 MB 24.4 MB/s eta 0:00:25     |██████████▉                     | 297.6 MB 25.5 MB/s eta 0:00:23     |███████████▋                    | 318.9 MB 28.7 MB/s eta 0:00:20     |███████████████▎                | 420.4 MB 762 kB/s eta 0:10:06     |████████████████▉               | 464.2 MB 150 kB/s eta 0:46:16     |██████████████████████████████▏ | 831.5 MB 752 kB/s eta 0:01:08\n",
      "\u001b[?25hCollecting torchvision>=0.5.0\n",
      "  Downloading torchvision-0.11.2-cp36-cp36m-manylinux1_x86_64.whl (23.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.3 MB 33.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dominate>=2.4.0\n",
      "  Downloading dominate-2.7.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting visdom>=0.1.8.8\n",
      "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 42.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 31.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/mate/miniconda3/envs/pix2pix/lib/python3.6/site-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in /home/mate/miniconda3/envs/pix2pix/lib/python3.6/site-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (0.8)\n",
      "Collecting pillow!=8.3.0,>=5.3.0\n",
      "  Downloading Pillow-8.4.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 33.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8 MB 33.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.4.0\n",
      "  Downloading torch-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 66 kB/s s eta 0:00:01     |██████████▍                     | 287.7 MB 20.9 MB/s eta 0:00:29     |████████████▌                   | 344.6 MB 33.0 MB/s eta 0:00:17     |███████████████████████████▉    | 767.4 MB 27.2 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 33.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s  eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: tornado in /home/mate/miniconda3/envs/pix2pix/lib/python3.6/site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (6.1)\n",
      "Requirement already satisfied: six in /home/mate/miniconda3/envs/pix2pix/lib/python3.6/site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting jsonpatch\n",
      "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
      "Collecting websocket-client\n",
      "  Downloading websocket_client-1.3.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 39.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting Click!=8.0.0,>=7.0\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 8.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
      "\u001b[K     |████████████████████████████████| 170 kB 38.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pathtools\n",
      "  Using cached pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
      "\u001b[K     |████████████████████████████████| 184 kB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/mate/miniconda3/envs/pix2pix/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 5)) (58.0.4)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.12.0\n",
      "  Downloading protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 41.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML\n",
      "  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "\u001b[K     |████████████████████████████████| 603 kB 29.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting psutil>=5.0.0\n",
      "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
      "\u001b[K     |████████████████████████████████| 280 kB 37.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/mate/miniconda3/envs/pix2pix/lib/python3.6/site-packages (from Click!=8.0.0,>=7.0->wandb->-r requirements.txt (line 5)) (4.8.3)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mate/miniconda3/envs/pix2pix/lib/python3.6/site-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2021.5.30)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 40.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/mate/miniconda3/envs/pix2pix/lib/python3.6/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb->-r requirements.txt (line 5)) (3.6.0)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting decorator<5,>=4.3\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Building wheels for collected packages: visdom, pathtools\n",
      "  Building wheel for visdom (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408218 sha256=83cc841c890386cb1ecd1851a9a5b7c2dc11192562ed58b01013a021ff2ec22e\n",
      "  Stored in directory: /home/mate/.cache/pip/wheels/28/ef/e5/80c89683cdfc87b2c1f772c61aefd1920cadcfb1677243981d\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=bc0d5fffccbd89562ac3c0c877a42b22d479b80b49019f1561dd57eecc214ec8\n",
      "  Stored in directory: /home/mate/.cache/pip/wheels/42/ea/90/e37d463fb3b03848bf715080595de62545266f53dd546b2497\n",
      "Successfully built visdom pathtools\n",
      "Installing collected packages: smmap, urllib3, numpy, jsonpointer, idna, gitdb, decorator, charset-normalizer, websocket-client, torch, setproctitle, sentry-sdk, scipy, requests, PyYAML, psutil, protobuf, pillow, pathtools, networkx, jsonpatch, GitPython, docker-pycreds, Click, appdirs, wandb, visdom, torchvision, dominate\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "Successfully installed Click-8.0.4 GitPython-3.1.18 PyYAML-6.0 appdirs-1.4.4 charset-normalizer-2.0.12 decorator-4.4.2 docker-pycreds-0.4.0 dominate-2.7.0 gitdb-4.0.9 idna-3.4 jsonpatch-1.32 jsonpointer-2.3 networkx-2.5.1 numpy-1.19.5 pathtools-0.1.2 pillow-8.4.0 protobuf-3.19.6 psutil-5.9.4 requests-2.27.1 scipy-1.5.4 sentry-sdk-1.16.0 setproctitle-1.2.3 smmap-5.0.0 torch-1.10.1 torchvision-0.11.2 urllib3-1.26.15 visdom-0.2.4 wandb-0.14.0 websocket-client-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8daqlgVhw29P"
   },
   "source": [
    "# Datasets\n",
    "\n",
    "Download one of the official datasets with:\n",
    "\n",
    "-   `bash ./datasets/download_pix2pix_dataset.sh [cityscapes, night2day, edges2handbags, edges2shoes, facades, maps]`\n",
    "\n",
    "Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrdOettJxaCc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified [facades]\n",
      "WARNING: timestamping does nothing in combination with -O. See the manual\n",
      "for details.\n",
      "\n",
      "--2023-03-15 17:10:59--  http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
      "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.244.190\n",
      "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.244.190|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30168306 (29M) [application/x-gzip]\n",
      "Saving to: ‘./datasets/facades.tar.gz’\n",
      "\n",
      "./datasets/facades. 100%[===================>]  28.77M  1.03MB/s    in 24s     \n",
      "\n",
      "2023-03-15 17:11:23 (1.21 MB/s) - ‘./datasets/facades.tar.gz’ saved [30168306/30168306]\n",
      "\n",
      "facades/\n",
      "facades/test/\n",
      "facades/test/27.jpg\n",
      "facades/test/5.jpg\n",
      "facades/test/72.jpg\n",
      "facades/test/1.jpg\n",
      "facades/test/10.jpg\n",
      "facades/test/100.jpg\n",
      "facades/test/101.jpg\n",
      "facades/test/102.jpg\n",
      "facades/test/103.jpg\n",
      "facades/test/104.jpg\n",
      "facades/test/105.jpg\n",
      "facades/test/106.jpg\n",
      "facades/test/11.jpg\n",
      "facades/test/12.jpg\n",
      "facades/test/13.jpg\n",
      "facades/test/14.jpg\n",
      "facades/test/15.jpg\n",
      "facades/test/16.jpg\n",
      "facades/test/17.jpg\n",
      "facades/test/18.jpg\n",
      "facades/test/19.jpg\n",
      "facades/test/2.jpg\n",
      "facades/test/20.jpg\n",
      "facades/test/21.jpg\n",
      "facades/test/22.jpg\n",
      "facades/test/23.jpg\n",
      "facades/test/24.jpg\n",
      "facades/test/25.jpg\n",
      "facades/test/26.jpg\n",
      "facades/test/50.jpg\n",
      "facades/test/51.jpg\n",
      "facades/test/52.jpg\n",
      "facades/test/53.jpg\n",
      "facades/test/54.jpg\n",
      "facades/test/55.jpg\n",
      "facades/test/56.jpg\n",
      "facades/test/57.jpg\n",
      "facades/test/58.jpg\n",
      "facades/test/59.jpg\n",
      "facades/test/6.jpg\n",
      "facades/test/60.jpg\n",
      "facades/test/61.jpg\n",
      "facades/test/62.jpg\n",
      "facades/test/63.jpg\n",
      "facades/test/64.jpg\n",
      "facades/test/65.jpg\n",
      "facades/test/66.jpg\n",
      "facades/test/67.jpg\n",
      "facades/test/68.jpg\n",
      "facades/test/69.jpg\n",
      "facades/test/7.jpg\n",
      "facades/test/70.jpg\n",
      "facades/test/71.jpg\n",
      "facades/test/73.jpg\n",
      "facades/test/74.jpg\n",
      "facades/test/75.jpg\n",
      "facades/test/76.jpg\n",
      "facades/test/77.jpg\n",
      "facades/test/78.jpg\n",
      "facades/test/79.jpg\n",
      "facades/test/8.jpg\n",
      "facades/test/80.jpg\n",
      "facades/test/81.jpg\n",
      "facades/test/82.jpg\n",
      "facades/test/83.jpg\n",
      "facades/test/84.jpg\n",
      "facades/test/85.jpg\n",
      "facades/test/86.jpg\n",
      "facades/test/87.jpg\n",
      "facades/test/88.jpg\n",
      "facades/test/89.jpg\n",
      "facades/test/9.jpg\n",
      "facades/test/90.jpg\n",
      "facades/test/91.jpg\n",
      "facades/test/92.jpg\n",
      "facades/test/93.jpg\n",
      "facades/test/94.jpg\n",
      "facades/test/95.jpg\n",
      "facades/test/96.jpg\n",
      "facades/test/97.jpg\n",
      "facades/test/98.jpg\n",
      "facades/test/99.jpg\n",
      "facades/test/28.jpg\n",
      "facades/test/29.jpg\n",
      "facades/test/3.jpg\n",
      "facades/test/30.jpg\n",
      "facades/test/31.jpg\n",
      "facades/test/32.jpg\n",
      "facades/test/33.jpg\n",
      "facades/test/34.jpg\n",
      "facades/test/35.jpg\n",
      "facades/test/36.jpg\n",
      "facades/test/37.jpg\n",
      "facades/test/38.jpg\n",
      "facades/test/39.jpg\n",
      "facades/test/4.jpg\n",
      "facades/test/40.jpg\n",
      "facades/test/41.jpg\n",
      "facades/test/42.jpg\n",
      "facades/test/43.jpg\n",
      "facades/test/44.jpg\n",
      "facades/test/45.jpg\n",
      "facades/test/46.jpg\n",
      "facades/test/47.jpg\n",
      "facades/test/48.jpg\n",
      "facades/test/49.jpg\n",
      "facades/train/\n",
      "facades/train/1.jpg\n",
      "facades/train/10.jpg\n",
      "facades/train/100.jpg\n",
      "facades/train/101.jpg\n",
      "facades/train/102.jpg\n",
      "facades/train/103.jpg\n",
      "facades/train/104.jpg\n",
      "facades/train/105.jpg\n",
      "facades/train/106.jpg\n",
      "facades/train/107.jpg\n",
      "facades/train/108.jpg\n",
      "facades/train/109.jpg\n",
      "facades/train/11.jpg\n",
      "facades/train/110.jpg\n",
      "facades/train/111.jpg\n",
      "facades/train/112.jpg\n",
      "facades/train/113.jpg\n",
      "facades/train/114.jpg\n",
      "facades/train/115.jpg\n",
      "facades/train/116.jpg\n",
      "facades/train/117.jpg\n",
      "facades/train/118.jpg\n",
      "facades/train/119.jpg\n",
      "facades/train/12.jpg\n",
      "facades/train/120.jpg\n",
      "facades/train/121.jpg\n",
      "facades/train/122.jpg\n",
      "facades/train/123.jpg\n",
      "facades/train/124.jpg\n",
      "facades/train/125.jpg\n",
      "facades/train/126.jpg\n",
      "facades/train/309.jpg\n",
      "facades/train/31.jpg\n",
      "facades/train/310.jpg\n",
      "facades/train/311.jpg\n",
      "facades/train/312.jpg\n",
      "facades/train/313.jpg\n",
      "facades/train/314.jpg\n",
      "facades/train/315.jpg\n",
      "facades/train/316.jpg\n",
      "facades/train/317.jpg\n",
      "facades/train/318.jpg\n",
      "facades/train/319.jpg\n",
      "facades/train/32.jpg\n",
      "facades/train/320.jpg\n",
      "facades/train/321.jpg\n",
      "facades/train/322.jpg\n",
      "facades/train/323.jpg\n",
      "facades/train/324.jpg\n",
      "facades/train/325.jpg\n",
      "facades/train/326.jpg\n",
      "facades/train/327.jpg\n",
      "facades/train/328.jpg\n",
      "facades/train/329.jpg\n",
      "facades/train/390.jpg\n",
      "facades/train/391.jpg\n",
      "facades/train/392.jpg\n",
      "facades/train/393.jpg\n",
      "facades/train/394.jpg\n",
      "facades/train/395.jpg\n",
      "facades/train/396.jpg\n",
      "facades/train/397.jpg\n",
      "facades/train/398.jpg\n",
      "facades/train/399.jpg\n",
      "facades/train/4.jpg\n",
      "facades/train/40.jpg\n",
      "facades/train/400.jpg\n",
      "facades/train/41.jpg\n",
      "facades/train/42.jpg\n",
      "facades/train/43.jpg\n",
      "facades/train/44.jpg\n",
      "facades/train/45.jpg\n",
      "facades/train/46.jpg\n",
      "facades/train/47.jpg\n",
      "facades/train/48.jpg\n",
      "facades/train/49.jpg\n",
      "facades/train/5.jpg\n",
      "facades/train/50.jpg\n",
      "facades/train/51.jpg\n",
      "facades/train/52.jpg\n",
      "facades/train/53.jpg\n",
      "facades/train/54.jpg\n",
      "facades/train/55.jpg\n",
      "facades/train/56.jpg\n",
      "facades/train/57.jpg\n",
      "facades/train/58.jpg\n",
      "facades/train/59.jpg\n",
      "facades/train/6.jpg\n",
      "facades/train/60.jpg\n",
      "facades/train/61.jpg\n",
      "facades/train/222.jpg\n",
      "facades/train/223.jpg\n",
      "facades/train/224.jpg\n",
      "facades/train/225.jpg\n",
      "facades/train/226.jpg\n",
      "facades/train/227.jpg\n",
      "facades/train/228.jpg\n",
      "facades/train/229.jpg\n",
      "facades/train/23.jpg\n",
      "facades/train/230.jpg\n",
      "facades/train/231.jpg\n",
      "facades/train/232.jpg\n",
      "facades/train/233.jpg\n",
      "facades/train/234.jpg\n",
      "facades/train/235.jpg\n",
      "facades/train/236.jpg\n",
      "facades/train/237.jpg\n",
      "facades/train/238.jpg\n",
      "facades/train/239.jpg\n",
      "facades/train/24.jpg\n",
      "facades/train/240.jpg\n",
      "facades/train/241.jpg\n",
      "facades/train/242.jpg\n",
      "facades/train/243.jpg\n",
      "facades/train/244.jpg\n",
      "facades/train/245.jpg\n",
      "facades/train/156.jpg\n",
      "facades/train/157.jpg\n",
      "facades/train/158.jpg\n",
      "facades/train/159.jpg\n",
      "facades/train/16.jpg\n",
      "facades/train/160.jpg\n",
      "facades/train/161.jpg\n",
      "facades/train/162.jpg\n",
      "facades/train/163.jpg\n",
      "facades/train/164.jpg\n",
      "facades/train/165.jpg\n",
      "facades/train/166.jpg\n",
      "facades/train/167.jpg\n",
      "facades/train/168.jpg\n",
      "facades/train/169.jpg\n",
      "facades/train/17.jpg\n",
      "facades/train/170.jpg\n",
      "facades/train/171.jpg\n",
      "facades/train/172.jpg\n",
      "facades/train/173.jpg\n",
      "facades/train/174.jpg\n",
      "facades/train/175.jpg\n",
      "facades/train/176.jpg\n",
      "facades/train/177.jpg\n",
      "facades/train/178.jpg\n",
      "facades/train/179.jpg\n",
      "facades/train/18.jpg\n",
      "facades/train/180.jpg\n",
      "facades/train/181.jpg\n",
      "facades/train/182.jpg\n",
      "facades/train/183.jpg\n",
      "facades/train/184.jpg\n",
      "facades/train/185.jpg\n",
      "facades/train/186.jpg\n",
      "facades/train/187.jpg\n",
      "facades/train/188.jpg\n",
      "facades/train/189.jpg\n",
      "facades/train/19.jpg\n",
      "facades/train/127.jpg\n",
      "facades/train/155.jpg\n",
      "facades/train/190.jpg\n",
      "facades/train/221.jpg\n",
      "facades/train/246.jpg\n",
      "facades/train/27.jpg\n",
      "facades/train/29.jpg\n",
      "facades/train/308.jpg\n",
      "facades/train/33.jpg\n",
      "facades/train/350.jpg\n",
      "facades/train/370.jpg\n",
      "facades/train/39.jpg\n",
      "facades/train/62.jpg\n",
      "facades/train/270.jpg\n",
      "facades/train/271.jpg\n",
      "facades/train/272.jpg\n",
      "facades/train/273.jpg\n",
      "facades/train/274.jpg\n",
      "facades/train/275.jpg\n",
      "facades/train/276.jpg\n",
      "facades/train/277.jpg\n",
      "facades/train/278.jpg\n",
      "facades/train/279.jpg\n",
      "facades/train/28.jpg\n",
      "facades/train/280.jpg\n",
      "facades/train/281.jpg\n",
      "facades/train/282.jpg\n",
      "facades/train/283.jpg\n",
      "facades/train/284.jpg\n",
      "facades/train/285.jpg\n",
      "facades/train/286.jpg\n",
      "facades/train/287.jpg\n",
      "facades/train/288.jpg\n",
      "facades/train/289.jpg\n",
      "facades/train/351.jpg\n",
      "facades/train/352.jpg\n",
      "facades/train/353.jpg\n",
      "facades/train/354.jpg\n",
      "facades/train/355.jpg\n",
      "facades/train/356.jpg\n",
      "facades/train/357.jpg\n",
      "facades/train/358.jpg\n",
      "facades/train/359.jpg\n",
      "facades/train/36.jpg\n",
      "facades/train/360.jpg\n",
      "facades/train/361.jpg\n",
      "facades/train/362.jpg\n",
      "facades/train/363.jpg\n",
      "facades/train/364.jpg\n",
      "facades/train/365.jpg\n",
      "facades/train/366.jpg\n",
      "facades/train/367.jpg\n",
      "facades/train/368.jpg\n",
      "facades/train/369.jpg\n",
      "facades/train/37.jpg\n",
      "facades/train/63.jpg\n",
      "facades/train/64.jpg\n",
      "facades/train/65.jpg\n",
      "facades/train/66.jpg\n",
      "facades/train/67.jpg\n",
      "facades/train/68.jpg\n",
      "facades/train/69.jpg\n",
      "facades/train/7.jpg\n",
      "facades/train/70.jpg\n",
      "facades/train/71.jpg\n",
      "facades/train/72.jpg\n",
      "facades/train/73.jpg\n",
      "facades/train/74.jpg\n",
      "facades/train/75.jpg\n",
      "facades/train/76.jpg\n",
      "facades/train/77.jpg\n",
      "facades/train/78.jpg\n",
      "facades/train/79.jpg\n",
      "facades/train/8.jpg\n",
      "facades/train/80.jpg\n",
      "facades/train/81.jpg\n",
      "facades/train/82.jpg\n",
      "facades/train/83.jpg\n",
      "facades/train/84.jpg\n",
      "facades/train/85.jpg\n",
      "facades/train/86.jpg\n",
      "facades/train/87.jpg\n",
      "facades/train/88.jpg\n",
      "facades/train/89.jpg\n",
      "facades/train/9.jpg\n",
      "facades/train/90.jpg\n",
      "facades/train/91.jpg\n",
      "facades/train/92.jpg\n",
      "facades/train/93.jpg\n",
      "facades/train/94.jpg\n",
      "facades/train/95.jpg\n",
      "facades/train/96.jpg\n",
      "facades/train/97.jpg\n",
      "facades/train/98.jpg\n",
      "facades/train/99.jpg\n",
      "facades/train/128.jpg\n",
      "facades/train/129.jpg\n",
      "facades/train/13.jpg\n",
      "facades/train/130.jpg\n",
      "facades/train/131.jpg\n",
      "facades/train/132.jpg\n",
      "facades/train/133.jpg\n",
      "facades/train/134.jpg\n",
      "facades/train/135.jpg\n",
      "facades/train/136.jpg\n",
      "facades/train/137.jpg\n",
      "facades/train/138.jpg\n",
      "facades/train/139.jpg\n",
      "facades/train/14.jpg\n",
      "facades/train/140.jpg\n",
      "facades/train/141.jpg\n",
      "facades/train/142.jpg\n",
      "facades/train/143.jpg\n",
      "facades/train/144.jpg\n",
      "facades/train/145.jpg\n",
      "facades/train/146.jpg\n",
      "facades/train/147.jpg\n",
      "facades/train/148.jpg\n",
      "facades/train/149.jpg\n",
      "facades/train/15.jpg\n",
      "facades/train/150.jpg\n",
      "facades/train/151.jpg\n",
      "facades/train/152.jpg\n",
      "facades/train/153.jpg\n",
      "facades/train/154.jpg\n",
      "facades/train/191.jpg\n",
      "facades/train/192.jpg\n",
      "facades/train/193.jpg\n",
      "facades/train/194.jpg\n",
      "facades/train/195.jpg\n",
      "facades/train/196.jpg\n",
      "facades/train/197.jpg\n",
      "facades/train/198.jpg\n",
      "facades/train/199.jpg\n",
      "facades/train/2.jpg\n",
      "facades/train/20.jpg\n",
      "facades/train/200.jpg\n",
      "facades/train/201.jpg\n",
      "facades/train/202.jpg\n",
      "facades/train/203.jpg\n",
      "facades/train/204.jpg\n",
      "facades/train/205.jpg\n",
      "facades/train/206.jpg\n",
      "facades/train/207.jpg\n",
      "facades/train/208.jpg\n",
      "facades/train/209.jpg\n",
      "facades/train/21.jpg\n",
      "facades/train/210.jpg\n",
      "facades/train/211.jpg\n",
      "facades/train/212.jpg\n",
      "facades/train/213.jpg\n",
      "facades/train/214.jpg\n",
      "facades/train/215.jpg\n",
      "facades/train/216.jpg\n",
      "facades/train/217.jpg\n",
      "facades/train/218.jpg\n",
      "facades/train/219.jpg\n",
      "facades/train/22.jpg\n",
      "facades/train/220.jpg\n",
      "facades/train/247.jpg\n",
      "facades/train/248.jpg\n",
      "facades/train/249.jpg\n",
      "facades/train/25.jpg\n",
      "facades/train/250.jpg\n",
      "facades/train/251.jpg\n",
      "facades/train/252.jpg\n",
      "facades/train/253.jpg\n",
      "facades/train/254.jpg\n",
      "facades/train/255.jpg\n",
      "facades/train/256.jpg\n",
      "facades/train/257.jpg\n",
      "facades/train/258.jpg\n",
      "facades/train/259.jpg\n",
      "facades/train/26.jpg\n",
      "facades/train/260.jpg\n",
      "facades/train/261.jpg\n",
      "facades/train/262.jpg\n",
      "facades/train/263.jpg\n",
      "facades/train/264.jpg\n",
      "facades/train/265.jpg\n",
      "facades/train/266.jpg\n",
      "facades/train/267.jpg\n",
      "facades/train/268.jpg\n",
      "facades/train/269.jpg\n",
      "facades/train/330.jpg\n",
      "facades/train/331.jpg\n",
      "facades/train/332.jpg\n",
      "facades/train/333.jpg\n",
      "facades/train/334.jpg\n",
      "facades/train/335.jpg\n",
      "facades/train/336.jpg\n",
      "facades/train/337.jpg\n",
      "facades/train/338.jpg\n",
      "facades/train/339.jpg\n",
      "facades/train/34.jpg\n",
      "facades/train/340.jpg\n",
      "facades/train/341.jpg\n",
      "facades/train/342.jpg\n",
      "facades/train/343.jpg\n",
      "facades/train/344.jpg\n",
      "facades/train/345.jpg\n",
      "facades/train/346.jpg\n",
      "facades/train/347.jpg\n",
      "facades/train/348.jpg\n",
      "facades/train/349.jpg\n",
      "facades/train/35.jpg\n",
      "facades/train/290.jpg\n",
      "facades/train/291.jpg\n",
      "facades/train/292.jpg\n",
      "facades/train/293.jpg\n",
      "facades/train/294.jpg\n",
      "facades/train/295.jpg\n",
      "facades/train/296.jpg\n",
      "facades/train/297.jpg\n",
      "facades/train/298.jpg\n",
      "facades/train/299.jpg\n",
      "facades/train/3.jpg\n",
      "facades/train/30.jpg\n",
      "facades/train/300.jpg\n",
      "facades/train/301.jpg\n",
      "facades/train/302.jpg\n",
      "facades/train/303.jpg\n",
      "facades/train/304.jpg\n",
      "facades/train/305.jpg\n",
      "facades/train/306.jpg\n",
      "facades/train/307.jpg\n",
      "facades/train/371.jpg\n",
      "facades/train/372.jpg\n",
      "facades/train/373.jpg\n",
      "facades/train/374.jpg\n",
      "facades/train/375.jpg\n",
      "facades/train/376.jpg\n",
      "facades/train/377.jpg\n",
      "facades/train/378.jpg\n",
      "facades/train/379.jpg\n",
      "facades/train/38.jpg\n",
      "facades/train/380.jpg\n",
      "facades/train/381.jpg\n",
      "facades/train/382.jpg\n",
      "facades/train/383.jpg\n",
      "facades/train/384.jpg\n",
      "facades/train/385.jpg\n",
      "facades/train/386.jpg\n",
      "facades/train/387.jpg\n",
      "facades/train/388.jpg\n",
      "facades/train/389.jpg\n",
      "facades/val/\n",
      "facades/val/30.jpg\n",
      "facades/val/50.jpg\n",
      "facades/val/73.jpg\n",
      "facades/val/1.jpg\n",
      "facades/val/10.jpg\n",
      "facades/val/100.jpg\n",
      "facades/val/11.jpg\n",
      "facades/val/12.jpg\n",
      "facades/val/13.jpg\n",
      "facades/val/14.jpg\n",
      "facades/val/15.jpg\n",
      "facades/val/16.jpg\n",
      "facades/val/17.jpg\n",
      "facades/val/18.jpg\n",
      "facades/val/19.jpg\n",
      "facades/val/2.jpg\n",
      "facades/val/20.jpg\n",
      "facades/val/21.jpg\n",
      "facades/val/22.jpg\n",
      "facades/val/23.jpg\n",
      "facades/val/24.jpg\n",
      "facades/val/25.jpg\n",
      "facades/val/26.jpg\n",
      "facades/val/27.jpg\n",
      "facades/val/28.jpg\n",
      "facades/val/29.jpg\n",
      "facades/val/3.jpg\n",
      "facades/val/51.jpg\n",
      "facades/val/52.jpg\n",
      "facades/val/53.jpg\n",
      "facades/val/54.jpg\n",
      "facades/val/55.jpg\n",
      "facades/val/56.jpg\n",
      "facades/val/57.jpg\n",
      "facades/val/58.jpg\n",
      "facades/val/59.jpg\n",
      "facades/val/6.jpg\n",
      "facades/val/60.jpg\n",
      "facades/val/61.jpg\n",
      "facades/val/62.jpg\n",
      "facades/val/63.jpg\n",
      "facades/val/64.jpg\n",
      "facades/val/65.jpg\n",
      "facades/val/66.jpg\n",
      "facades/val/67.jpg\n",
      "facades/val/68.jpg\n",
      "facades/val/69.jpg\n",
      "facades/val/7.jpg\n",
      "facades/val/70.jpg\n",
      "facades/val/71.jpg\n",
      "facades/val/72.jpg\n",
      "facades/val/74.jpg\n",
      "facades/val/75.jpg\n",
      "facades/val/76.jpg\n",
      "facades/val/77.jpg\n",
      "facades/val/78.jpg\n",
      "facades/val/79.jpg\n",
      "facades/val/8.jpg\n",
      "facades/val/80.jpg\n",
      "facades/val/81.jpg\n",
      "facades/val/82.jpg\n",
      "facades/val/83.jpg\n",
      "facades/val/84.jpg\n",
      "facades/val/85.jpg\n",
      "facades/val/86.jpg\n",
      "facades/val/87.jpg\n",
      "facades/val/88.jpg\n",
      "facades/val/89.jpg\n",
      "facades/val/9.jpg\n",
      "facades/val/90.jpg\n",
      "facades/val/91.jpg\n",
      "facades/val/92.jpg\n",
      "facades/val/93.jpg\n",
      "facades/val/94.jpg\n",
      "facades/val/95.jpg\n",
      "facades/val/96.jpg\n",
      "facades/val/97.jpg\n",
      "facades/val/98.jpg\n",
      "facades/val/99.jpg\n",
      "facades/val/31.jpg\n",
      "facades/val/32.jpg\n",
      "facades/val/33.jpg\n",
      "facades/val/34.jpg\n",
      "facades/val/35.jpg\n",
      "facades/val/36.jpg\n",
      "facades/val/37.jpg\n",
      "facades/val/38.jpg\n",
      "facades/val/39.jpg\n",
      "facades/val/4.jpg\n",
      "facades/val/40.jpg\n",
      "facades/val/41.jpg\n",
      "facades/val/42.jpg\n",
      "facades/val/43.jpg\n",
      "facades/val/44.jpg\n",
      "facades/val/45.jpg\n",
      "facades/val/46.jpg\n",
      "facades/val/47.jpg\n",
      "facades/val/48.jpg\n",
      "facades/val/49.jpg\n",
      "facades/val/5.jpg\n"
     ]
    }
   ],
   "source": [
    "!bash ./datasets/download_pix2pix_dataset.sh facades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdUz4116xhpm"
   },
   "source": [
    "# Pretrained models\n",
    "\n",
    "Download one of the official pretrained models with:\n",
    "\n",
    "-   `bash ./scripts/download_pix2pix_model.sh [edges2shoes, sat2map, map2sat, facades_label2photo, and day2night]`\n",
    "\n",
    "Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GC2DEP4M0OsS"
   },
   "outputs": [],
   "source": [
    "!bash ./scripts/download_pix2pix_model.sh facades_label2photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFw1kDQBx3LN"
   },
   "source": [
    "# Training\n",
    "\n",
    "-   `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA`\n",
    "\n",
    "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. Add `--direction BtoA` if you want to train a model to transfrom from class B to A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sp7TCT2x9dB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./datasets/facades            \t[default: None]\n",
      "             dataset_mode: aligned                       \n",
      "                direction: BtoA                          \t[default: AtoB]\n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: -1                            \t[default: 1]\n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: vanilla                       \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                lambda_L1: 100.0                         \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \t[default: cycle_gan]\n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: facades_pix2pix               \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: batch                         \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "dataset [AlignedDataset] was created\n",
      "The number of training images = 400\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 54.414 M\n",
      "[Network D] Total number of parameters : 2.769 M\n",
      "-----------------------------------------------\n",
      "create web directory ./checkpoints/facades_pix2pix/web...\n",
      "/home/mate/miniconda3/envs/pix2pix-gpu/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 1, iters: 100, time: 0.075, data: 0.117) G_GAN: 1.896 G_L1: 36.239 D_real: 0.248 D_fake: 0.667 \n",
      "(epoch: 1, iters: 200, time: 0.081, data: 0.001) G_GAN: 3.055 G_L1: 29.083 D_real: 0.208 D_fake: 0.081 \n",
      "(epoch: 1, iters: 300, time: 0.077, data: 0.001) G_GAN: 1.702 G_L1: 39.090 D_real: 0.049 D_fake: 1.738 \n",
      "(epoch: 1, iters: 400, time: 0.149, data: 0.001) G_GAN: 2.631 G_L1: 38.243 D_real: 0.241 D_fake: 0.065 \n",
      "End of epoch 1 / 200 \t Time Taken: 23 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 2, iters: 100, time: 0.071, data: 0.107) G_GAN: 4.274 G_L1: 36.205 D_real: 0.074 D_fake: 0.042 \n",
      "(epoch: 2, iters: 200, time: 0.071, data: 0.001) G_GAN: 2.568 G_L1: 36.041 D_real: 0.022 D_fake: 0.153 \n",
      "(epoch: 2, iters: 300, time: 0.069, data: 0.001) G_GAN: 0.792 G_L1: 24.065 D_real: 2.558 D_fake: 0.062 \n",
      "(epoch: 2, iters: 400, time: 0.139, data: 0.001) G_GAN: 1.537 G_L1: 30.416 D_real: 0.938 D_fake: 0.060 \n",
      "End of epoch 2 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 3, iters: 100, time: 0.074, data: 0.099) G_GAN: 2.811 G_L1: 26.611 D_real: 0.011 D_fake: 1.395 \n",
      "(epoch: 3, iters: 200, time: 0.069, data: 0.001) G_GAN: 2.012 G_L1: 30.109 D_real: 0.907 D_fake: 0.041 \n",
      "(epoch: 3, iters: 300, time: 0.068, data: 0.001) G_GAN: 3.426 G_L1: 39.339 D_real: 0.006 D_fake: 0.280 \n",
      "(epoch: 3, iters: 400, time: 0.154, data: 0.001) G_GAN: 2.639 G_L1: 32.757 D_real: 0.196 D_fake: 0.237 \n",
      "End of epoch 3 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 4, iters: 100, time: 0.068, data: 0.112) G_GAN: 2.139 G_L1: 34.621 D_real: 0.797 D_fake: 0.062 \n",
      "(epoch: 4, iters: 200, time: 0.073, data: 0.001) G_GAN: 2.978 G_L1: 32.355 D_real: 0.034 D_fake: 0.426 \n",
      "(epoch: 4, iters: 300, time: 0.073, data: 0.001) G_GAN: 2.483 G_L1: 26.471 D_real: 3.312 D_fake: 0.014 \n",
      "(epoch: 4, iters: 400, time: 0.138, data: 0.001) G_GAN: 2.342 G_L1: 49.572 D_real: 0.001 D_fake: 0.380 \n",
      "End of epoch 4 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 5, iters: 100, time: 0.075, data: 0.121) G_GAN: 2.675 G_L1: 56.169 D_real: 0.004 D_fake: 0.218 \n",
      "(epoch: 5, iters: 200, time: 0.075, data: 0.001) G_GAN: 1.035 G_L1: 27.798 D_real: 0.254 D_fake: 0.237 \n",
      "(epoch: 5, iters: 300, time: 0.075, data: 0.001) G_GAN: 4.505 G_L1: 46.117 D_real: 0.027 D_fake: 0.020 \n",
      "(epoch: 5, iters: 400, time: 0.139, data: 0.001) G_GAN: 2.922 G_L1: 42.263 D_real: 0.038 D_fake: 0.062 \n",
      "saving the model at the end of epoch 5, iters 2000\n",
      "End of epoch 5 / 200 \t Time Taken: 19 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 6, iters: 100, time: 0.073, data: 0.102) G_GAN: 0.782 G_L1: 23.061 D_real: 0.743 D_fake: 0.782 \n",
      "(epoch: 6, iters: 200, time: 0.069, data: 0.001) G_GAN: 1.846 G_L1: 38.657 D_real: 0.806 D_fake: 0.102 \n",
      "(epoch: 6, iters: 300, time: 0.068, data: 0.001) G_GAN: 3.427 G_L1: 58.916 D_real: 0.007 D_fake: 0.164 \n",
      "(epoch: 6, iters: 400, time: 0.138, data: 0.001) G_GAN: 2.541 G_L1: 31.475 D_real: 0.126 D_fake: 0.321 \n",
      "End of epoch 6 / 200 \t Time Taken: 17 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 7, iters: 100, time: 0.074, data: 0.096) G_GAN: 0.971 G_L1: 32.148 D_real: 2.156 D_fake: 0.107 \n",
      "(epoch: 7, iters: 200, time: 0.076, data: 0.001) G_GAN: 2.755 G_L1: 35.627 D_real: 0.092 D_fake: 0.701 \n",
      "(epoch: 7, iters: 300, time: 0.070, data: 0.002) G_GAN: 2.021 G_L1: 30.949 D_real: 0.014 D_fake: 0.225 \n",
      "(epoch: 7, iters: 400, time: 0.135, data: 0.002) G_GAN: 3.309 G_L1: 34.248 D_real: 0.248 D_fake: 0.061 \n",
      "End of epoch 7 / 200 \t Time Taken: 17 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 8, iters: 100, time: 0.070, data: 0.108) G_GAN: 2.893 G_L1: 50.309 D_real: 0.003 D_fake: 0.306 \n",
      "(epoch: 8, iters: 200, time: 0.073, data: 0.001) G_GAN: 2.499 G_L1: 67.657 D_real: 0.001 D_fake: 0.391 \n",
      "(epoch: 8, iters: 300, time: 0.072, data: 0.001) G_GAN: 5.683 G_L1: 36.894 D_real: 0.007 D_fake: 1.395 \n",
      "(epoch: 8, iters: 400, time: 0.138, data: 0.001) G_GAN: 1.677 G_L1: 34.235 D_real: 0.106 D_fake: 0.322 \n",
      "End of epoch 8 / 200 \t Time Taken: 17 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 9, iters: 100, time: 0.071, data: 0.102) G_GAN: 4.073 G_L1: 30.374 D_real: 0.476 D_fake: 0.011 \n",
      "(epoch: 9, iters: 200, time: 0.068, data: 0.001) G_GAN: 3.143 G_L1: 42.161 D_real: 0.001 D_fake: 0.241 \n",
      "(epoch: 9, iters: 300, time: 0.075, data: 0.001) G_GAN: 1.727 G_L1: 35.027 D_real: 0.818 D_fake: 0.068 \n",
      "(epoch: 9, iters: 400, time: 0.154, data: 0.001) G_GAN: 2.519 G_L1: 29.512 D_real: 2.498 D_fake: 0.026 \n",
      "End of epoch 9 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 10, iters: 100, time: 0.072, data: 0.103) G_GAN: 0.889 G_L1: 25.610 D_real: 0.847 D_fake: 0.218 \n",
      "(epoch: 10, iters: 200, time: 0.080, data: 0.001) G_GAN: 3.716 G_L1: 40.895 D_real: 0.016 D_fake: 0.036 \n",
      "(epoch: 10, iters: 300, time: 0.072, data: 0.001) G_GAN: 3.438 G_L1: 40.977 D_real: 0.014 D_fake: 0.181 \n",
      "(epoch: 10, iters: 400, time: 0.149, data: 0.001) G_GAN: 2.151 G_L1: 30.076 D_real: 0.467 D_fake: 0.080 \n",
      "saving the model at the end of epoch 10, iters 4000\n",
      "End of epoch 10 / 200 \t Time Taken: 19 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 11, iters: 100, time: 0.076, data: 0.098) G_GAN: 4.305 G_L1: 43.860 D_real: 0.008 D_fake: 0.033 \n",
      "(epoch: 11, iters: 200, time: 0.074, data: 0.001) G_GAN: 2.803 G_L1: 49.735 D_real: 0.001 D_fake: 0.167 \n",
      "(epoch: 11, iters: 300, time: 0.070, data: 0.001) G_GAN: 2.132 G_L1: 30.028 D_real: 0.070 D_fake: 0.744 \n",
      "(epoch: 11, iters: 400, time: 0.163, data: 0.001) G_GAN: 2.747 G_L1: 41.099 D_real: 0.030 D_fake: 0.152 \n",
      "End of epoch 11 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 12, iters: 100, time: 0.074, data: 0.103) G_GAN: 4.504 G_L1: 41.201 D_real: 0.037 D_fake: 0.016 \n",
      "(epoch: 12, iters: 200, time: 0.076, data: 0.001) G_GAN: 3.166 G_L1: 43.421 D_real: 0.007 D_fake: 0.088 \n",
      "(epoch: 12, iters: 300, time: 0.077, data: 0.003) G_GAN: 2.419 G_L1: 40.017 D_real: 0.178 D_fake: 0.184 \n",
      "(epoch: 12, iters: 400, time: 0.155, data: 0.001) G_GAN: 1.289 G_L1: 35.530 D_real: 0.377 D_fake: 0.220 \n",
      "End of epoch 12 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 13, iters: 100, time: 0.073, data: 0.110) G_GAN: 2.049 G_L1: 30.405 D_real: 0.192 D_fake: 0.381 \n",
      "(epoch: 13, iters: 200, time: 0.073, data: 0.001) G_GAN: 3.318 G_L1: 40.187 D_real: 0.235 D_fake: 0.073 \n",
      "saving the latest model (epoch 13, total_iters 5000)\n",
      "(epoch: 13, iters: 300, time: 0.075, data: 0.002) G_GAN: 2.966 G_L1: 39.266 D_real: 0.008 D_fake: 0.583 \n",
      "(epoch: 13, iters: 400, time: 0.143, data: 0.001) G_GAN: 3.325 G_L1: 45.292 D_real: 0.012 D_fake: 0.031 \n",
      "End of epoch 13 / 200 \t Time Taken: 19 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 14, iters: 100, time: 0.072, data: 0.130) G_GAN: 2.808 G_L1: 39.284 D_real: 0.122 D_fake: 0.134 \n",
      "(epoch: 14, iters: 200, time: 0.071, data: 0.001) G_GAN: 2.102 G_L1: 21.792 D_real: 0.248 D_fake: 1.732 \n",
      "(epoch: 14, iters: 300, time: 0.072, data: 0.001) G_GAN: 2.401 G_L1: 28.326 D_real: 1.001 D_fake: 0.031 \n",
      "(epoch: 14, iters: 400, time: 0.162, data: 0.001) G_GAN: 3.466 G_L1: 34.227 D_real: 0.388 D_fake: 0.023 \n",
      "End of epoch 14 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 15, iters: 100, time: 0.075, data: 0.121) G_GAN: 0.764 G_L1: 27.953 D_real: 0.539 D_fake: 0.230 \n",
      "(epoch: 15, iters: 200, time: 0.075, data: 0.001) G_GAN: 4.457 G_L1: 34.642 D_real: 0.016 D_fake: 0.031 \n",
      "(epoch: 15, iters: 300, time: 0.073, data: 0.001) G_GAN: 2.541 G_L1: 33.207 D_real: 0.154 D_fake: 0.087 \n",
      "(epoch: 15, iters: 400, time: 0.150, data: 0.001) G_GAN: 3.274 G_L1: 37.392 D_real: 0.304 D_fake: 0.044 \n",
      "saving the model at the end of epoch 15, iters 6000\n",
      "End of epoch 15 / 200 \t Time Taken: 19 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 16, iters: 100, time: 0.075, data: 0.120) G_GAN: 2.193 G_L1: 45.493 D_real: 0.007 D_fake: 0.234 \n",
      "(epoch: 16, iters: 200, time: 0.073, data: 0.001) G_GAN: 2.282 G_L1: 29.601 D_real: 0.055 D_fake: 0.325 \n",
      "(epoch: 16, iters: 300, time: 0.076, data: 0.001) G_GAN: 1.900 G_L1: 25.201 D_real: 0.181 D_fake: 0.213 \n",
      "(epoch: 16, iters: 400, time: 0.151, data: 0.001) G_GAN: 3.099 G_L1: 27.651 D_real: 0.036 D_fake: 0.087 \n",
      "End of epoch 16 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 17, iters: 100, time: 0.072, data: 0.135) G_GAN: 1.796 G_L1: 28.676 D_real: 0.387 D_fake: 0.253 \n",
      "(epoch: 17, iters: 200, time: 0.073, data: 0.001) G_GAN: 3.229 G_L1: 26.269 D_real: 0.280 D_fake: 0.082 \n",
      "(epoch: 17, iters: 300, time: 0.071, data: 0.002) G_GAN: 3.163 G_L1: 35.067 D_real: 0.074 D_fake: 0.047 \n",
      "(epoch: 17, iters: 400, time: 0.155, data: 0.001) G_GAN: 0.729 G_L1: 24.215 D_real: 2.386 D_fake: 0.152 \n",
      "End of epoch 17 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 18, iters: 100, time: 0.073, data: 0.116) G_GAN: 2.380 G_L1: 35.000 D_real: 0.189 D_fake: 0.138 \n",
      "(epoch: 18, iters: 200, time: 0.073, data: 0.001) G_GAN: 1.746 G_L1: 35.542 D_real: 0.519 D_fake: 0.327 \n",
      "(epoch: 18, iters: 300, time: 0.076, data: 0.002) G_GAN: 3.048 G_L1: 28.462 D_real: 1.113 D_fake: 0.016 \n",
      "(epoch: 18, iters: 400, time: 0.161, data: 0.001) G_GAN: 3.179 G_L1: 31.944 D_real: 0.056 D_fake: 0.058 \n",
      "End of epoch 18 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 19, iters: 100, time: 0.068, data: 0.111) G_GAN: 2.785 G_L1: 34.149 D_real: 0.905 D_fake: 0.029 \n",
      "(epoch: 19, iters: 200, time: 0.072, data: 0.001) G_GAN: 3.200 G_L1: 49.799 D_real: 0.002 D_fake: 1.505 \n",
      "(epoch: 19, iters: 300, time: 0.073, data: 0.001) G_GAN: 2.350 G_L1: 29.530 D_real: 0.006 D_fake: 0.812 \n",
      "(epoch: 19, iters: 400, time: 0.180, data: 0.001) G_GAN: 2.287 G_L1: 34.358 D_real: 0.036 D_fake: 0.353 \n",
      "End of epoch 19 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 20, iters: 100, time: 0.077, data: 0.107) G_GAN: 1.382 G_L1: 28.734 D_real: 0.127 D_fake: 0.432 \n",
      "(epoch: 20, iters: 200, time: 0.075, data: 0.001) G_GAN: 1.245 G_L1: 23.036 D_real: 3.074 D_fake: 0.046 \n",
      "(epoch: 20, iters: 300, time: 0.076, data: 0.001) G_GAN: 3.099 G_L1: 39.350 D_real: 0.060 D_fake: 0.078 \n",
      "(epoch: 20, iters: 400, time: 0.145, data: 0.001) G_GAN: 3.726 G_L1: 34.910 D_real: 0.169 D_fake: 0.040 \n",
      "saving the model at the end of epoch 20, iters 8000\n",
      "End of epoch 20 / 200 \t Time Taken: 19 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 21, iters: 100, time: 0.075, data: 0.111) G_GAN: 2.008 G_L1: 39.825 D_real: 0.026 D_fake: 0.227 \n",
      "(epoch: 21, iters: 200, time: 0.072, data: 0.001) G_GAN: 3.456 G_L1: 48.599 D_real: 0.001 D_fake: 0.059 \n",
      "(epoch: 21, iters: 300, time: 0.073, data: 0.001) G_GAN: 2.808 G_L1: 58.100 D_real: 0.000 D_fake: 0.231 \n",
      "(epoch: 21, iters: 400, time: 0.163, data: 0.001) G_GAN: 1.954 G_L1: 36.111 D_real: 0.029 D_fake: 0.331 \n",
      "End of epoch 21 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 22, iters: 100, time: 0.069, data: 0.113) G_GAN: 0.454 G_L1: 25.131 D_real: 1.412 D_fake: 0.155 \n",
      "(epoch: 22, iters: 200, time: 0.077, data: 0.001) G_GAN: 2.011 G_L1: 26.215 D_real: 0.160 D_fake: 0.239 \n",
      "(epoch: 22, iters: 300, time: 0.072, data: 0.001) G_GAN: 2.860 G_L1: 37.328 D_real: 0.003 D_fake: 0.170 \n",
      "(epoch: 22, iters: 400, time: 0.160, data: 0.001) G_GAN: 3.167 G_L1: 33.145 D_real: 0.223 D_fake: 0.058 \n",
      "End of epoch 22 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 23, iters: 100, time: 0.069, data: 0.106) G_GAN: 2.613 G_L1: 31.915 D_real: 0.156 D_fake: 0.084 \n",
      "(epoch: 23, iters: 200, time: 0.068, data: 0.001) G_GAN: 1.207 G_L1: 35.944 D_real: 0.654 D_fake: 0.126 \n",
      "(epoch: 23, iters: 300, time: 0.076, data: 0.001) G_GAN: 3.518 G_L1: 34.368 D_real: 0.007 D_fake: 0.033 \n",
      "(epoch: 23, iters: 400, time: 0.151, data: 0.001) G_GAN: 3.243 G_L1: 34.685 D_real: 0.234 D_fake: 0.042 \n",
      "End of epoch 23 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 24, iters: 100, time: 0.073, data: 0.114) G_GAN: 2.700 G_L1: 27.252 D_real: 0.209 D_fake: 0.427 \n",
      "(epoch: 24, iters: 200, time: 0.075, data: 0.001) G_GAN: 1.768 G_L1: 26.736 D_real: 0.056 D_fake: 0.561 \n",
      "(epoch: 24, iters: 300, time: 0.071, data: 0.001) G_GAN: 1.636 G_L1: 30.110 D_real: 0.465 D_fake: 0.468 \n",
      "(epoch: 24, iters: 400, time: 0.164, data: 0.001) G_GAN: 0.837 G_L1: 21.473 D_real: 2.599 D_fake: 0.099 \n",
      "End of epoch 24 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 25, iters: 100, time: 0.069, data: 0.111) G_GAN: 2.769 G_L1: 29.732 D_real: 0.086 D_fake: 0.124 \n",
      "(epoch: 25, iters: 200, time: 0.073, data: 0.001) G_GAN: 3.795 G_L1: 32.323 D_real: 0.211 D_fake: 0.026 \n",
      "(epoch: 25, iters: 300, time: 0.070, data: 0.001) G_GAN: 2.568 G_L1: 31.560 D_real: 0.036 D_fake: 0.338 \n",
      "(epoch: 25, iters: 400, time: 0.166, data: 0.001) G_GAN: 2.237 G_L1: 31.817 D_real: 0.069 D_fake: 0.163 \n",
      "saving the latest model (epoch 25, total_iters 10000)\n",
      "saving the model at the end of epoch 25, iters 10000\n",
      "End of epoch 25 / 200 \t Time Taken: 20 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 26, iters: 100, time: 0.075, data: 0.108) G_GAN: 2.618 G_L1: 27.827 D_real: 0.013 D_fake: 0.887 \n",
      "(epoch: 26, iters: 200, time: 0.072, data: 0.001) G_GAN: 1.325 G_L1: 27.884 D_real: 0.377 D_fake: 0.248 \n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 51, in <module>\n",
      "    model.set_input(data)         # unpack data from dataset and apply preprocessing\n",
      "  File \"/mnt/d/projects/cardiac_gan/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py\", line 83, in set_input\n",
      "    self.real_B = input['B' if AtoB else 'A'].to(self.device)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA --display_id -1 --gpu_ids 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UkcaFZiyASl"
   },
   "source": [
    "# Testing\n",
    "\n",
    "-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`\n",
    "\n",
    "Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.\n",
    "\n",
    "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
    "> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.\n",
    "\n",
    "> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).\n",
    "\n",
    "> See a list of currently available models at ./scripts/download_pix2pix_model.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mey7o6j-0368"
   },
   "outputs": [],
   "source": [
    "!ls checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCsKkEq0yGh0"
   },
   "outputs": [],
   "source": [
    "!python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_label2photo_pretrained --use_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzSKIPUByfiN"
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Mgg8raPyizq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_fake_B.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0G3oVH9DyqLQ"
   },
   "outputs": [],
   "source": [
    "img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_A.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErK5OC1j1LH4"
   },
   "outputs": [],
   "source": [
    "img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_B.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py --name se_test --dataset_mode se --dataroot '/mnt/d/projects/cardiac_gan/OneDrive_2023-03-16/SE dataset/se/2022_11_22/' --direction BtoA --gpu_ids 0 --preprocess none --use_wandb --wandb_project_name heartgan --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /mnt/d/projects/cardiac_gan/OneDrive_2023-03-16/SE dataset/se/test/\t[default: None]\n",
      "             dataset_mode: se                            \t[default: unaligned]\n",
      "                direction: BtoA                          \t[default: AtoB]\n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 1                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.5                           \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "             mapping_only: False                         \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: se_test                       \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "              observer_id: 1                             \n",
      "                output_nc: 1                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: none                          \t[default: resize_and_crop]\n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                    split: train                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "Found 21 dcm files with contours and 4 contours files.\n",
      "Found 0 uncorrected dcm files.\n",
      "Found 0 contrast dcm files.\n",
      "Couldn't identify contours file for  58 dcm files (not including uncorrected or contrast files).\n",
      "dataset [SeDataset] was created\n",
      "The number of training images = 21\n"
     ]
    }
   ],
   "source": [
    "!python train.py --name se_test --dataset_mode se --dataroot '/mnt/d/projects/cardiac_gan/OneDrive_2023-03-16/SE dataset/se/test/' --direction BtoA --gpu_ids 0 --preprocess none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --name se_test --dataset_mode se --dataroot '/mnt/d/projects/cardiac_gan/OneDrive_2023-03-16/SE dataset/se/2022_11_22/' --direction BtoA --gpu_ids 0 --preprocess none --use_wandb --wandb_project_name heartgan --verbose --display_freq 400 --model pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mate/miniconda3/envs/pix2pix-gpu/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 dcm files with contours and 4 contours files.\n",
      "Found 0 uncorrected dcm files.\n",
      "Found 0 contrast dcm files.\n",
      "Couldn't identify contours file for  58 dcm files (not including uncorrected or contrast files).\n"
     ]
    }
   ],
   "source": [
    "import mapping_dataset\n",
    "\n",
    "dataset = mapping_dataset.MappingDatasetAlbu(root='/mnt/d/projects/cardiac_gan/OneDrive_2023-03-16/SE dataset/se/test/', transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n",
      "(1, 256, 256)\n",
      "[0. 1. 2.]\n",
      "[0. 1. 2.]\n",
      "2.0\n",
      "[0. 1. 2.]\n",
      "0.0\n",
      "[0. 1. 2.]\n",
      "[0. 1. 2.]\n",
      "[0.         0.33333334 0.6666667 ]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "print(dataset.__getitem__(0)[1].shape)\n",
    "\n",
    "item = dataset.__getitem__(0)[1]\n",
    "\n",
    "print(max(dataset.__getitem__(0)[1].flatten()))\n",
    "print(min(dataset.__getitem__(0)[1].flatten()))\n",
    "print(np.unique(dataset.__getitem__(0)[1].flatten()))\n",
    "#item = item/3\n",
    "print(np.unique(item.flatten()))\n",
    "cv2.imshow('test', item.transpose(1,2,0))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "pix2pix",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
